{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Translation with nn.Transformer and torchtext\n",
    "\n",
    "Based on the tutorial: https://pytorch.org/tutorials/beginner/translation_transformer.html\n",
    "\n",
    "This notebook provides an introduction to language translation using the `nn.Transformer` module and `torchtext` in PyTorch. It demonstrates how to build and train a transformer model to translate sentences from German to English using the Multi30k dataset.\n",
    "\n",
    "The dataset is loaded using `torchtext.datasets.Multi30k` and tokenized using the `spacy` library. The data is then processed to build vocabularies for both the source (German) and target (English) languages. The processed data is split into training and validation sets, which are loaded using `DataLoader` with a specified batch size.\n",
    "\n",
    "A transformer model is defined using `torch.nn.Transformer`, consisting of an encoder and a decoder. The model is trained using the Adam optimizer and cross-entropy loss function. The training loop iterates over the dataset for a specified number of epochs, updating the model parameters to minimize the loss.\n",
    "\n",
    "Additionally, the notebook includes code to save the trained model's state dictionary to a file and load it back for inference. This allows for the model to be reused without retraining.\n",
    "\n",
    "The notebook concludes with a section on visualizing the model's translations for a sentence provided by user input, offering a qualitative assessment of the model's performance.\n",
    "\n",
    "Most of the boilerplate for the training is handled by the manager class. This includes printing metrics during training and periodically saving the model, allowing training to be resumed from the last checkpoint.\n",
    "\n",
    "The manager executes the model by passing only the input (because the target should not be passed by default). To allow passing the target (the model uses the target, except for the last token, because it iterates over the tokens to try to generate the next token), the collation function was changed to return a list of tuples `text_input/target_input` to be used by the executor. Furthermore, the `pad_sequence` used in the original tutorial outputs a tensor in which the first dimension corresponds to the tokens and not the batch items (which became the second dimension), but the manager expects a tuple of lists or tensors in which the length corresponds to the batch length (the loss and accuracy metrics are calculated per record, because the last batch may not be full and would have an incorrect weight over those metrics if the number of batches was used).\n",
    "\n",
    "To solve this, the dimensions of the text and target tensors are transposed (dimensions 0 and 1) in the collation function, which determines the batches returned by the dataloader, and then transposed again, using the same dimensions, after separating the inputs and targets. This returns them to their original forms, to then combine in a single tensor (one for the text input, another for the target).\n",
    "\n",
    "*Important:* The executor expects a batch in which each item in the input corresponds to a record in the dataset. So, a list of tuples `(text, target)` is allowed, but not a tuple of lists `(text[], target[])`. The executor may perform this conversion if necessary (as done in `MyBatchExecutor`).\n",
    "\n",
    "`MyBatchExecutor` is a custom executor defined in this notebook to handle the specific requirements of processing and running batches of text data through the transformer model. It expects a batch of tuples of `text_input/target_input` sentences, with the `target_input` sentence generated from the target, but without the last token (`EOS_IDX`).\n",
    "\n",
    "Key Responsibilities of `MyBatchExecutor`:\n",
    "\n",
    "1. **Input Processing**: Converts the input/target sentences into tensors suitable for the transformer model. This includes tokenizing the sentences, creating source and target tensors, and generating masks required by the transformer.\n",
    "2. **Model Execution**: Runs the transformer model with the processed input tensors to generate the output translations.\n",
    "3. **Output Handling**: Processes the model's output to produce the final translated sentences.\n",
    "\n",
    "A custom loss function was defined that calls `CrossEntropyLoss` under the hood after reshaping the output and target tensors and transposing the target (to have the tokens in dimension 0 and the different items of the batch in dimension 1). This is necessary because the collation function transposes the inputs and targets, but the executor transposes them back. Alternatively, the output of the executor could be transposed again to make dimension 0 correspond to the batch items.\n",
    "\n",
    "The evaluator, `MyEvaluator`, uses the `greedy_decode` and `translate` functions from the original tutorial. It generates tokens until `EOS_IDX` (end of sentence) is found or the maximum number of tokens is generated, and then returns the corresponding text. The evaluator is called by `manager.evaluate(input)`, where `input` is the text to be translated, and the output is the translated text.\n",
    "\n",
    "The accuracy calculator class is currently set to always return 0.5 (50%). The original tutorial evaluated only the loss, and not the accuracy, so the accuracy was not considered here. The accuracy calculator can be modified to measure the translation accuracy by comparing the model's translations with the true target sentences. For an example, see the accuracy calculator in [seq2seq_translation.ipynb](./seq2seq_translation.ipynb).\n",
    "\n",
    "A custom metrics class was used to calculate and display metrics related to the losses, which were then saved to a PDF file (`REPORT_PATH`). You can find implementation examples of various metrics classes in [lib/metrics.py](../lib/metrics.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = 'translation_transformer'\n",
    "SAVE_PATH = f'data/test/train/{NAME}-checkpoint.pth'\n",
    "REPORT_PATH = f'data/test/train/{NAME}-report.pdf'\n",
    "DATA_DIR = 'data/multi30k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sourcing and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pip install -U torchdata\n",
    "pip install -U spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> typing.Iterator[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(\n",
    "        root=DATA_DIR,\n",
    "        split='train',\n",
    "        language_pair=(SRC_LANGUAGE, TGT_LANGUAGE), # type: ignore\n",
    "    )\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(\n",
    "        yield_tokens(train_iter, ln),\n",
    "        min_freq=1,\n",
    "        specials=special_symbols,\n",
    "        special_first=True)\n",
    "\n",
    "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Network using Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((\n",
    "        torch.tensor([BOS_IDX]),\n",
    "        torch.tensor(token_ids),\n",
    "        torch.tensor([EOS_IDX]),\n",
    "    ))\n",
    "\n",
    "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(\n",
    "        token_transform[ln], #Tokenization\n",
    "        vocab_transform[ln], #Numericalization\n",
    "        tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        tgt = text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(src)\n",
    "        tgt_batch.append(tgt)\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "\n",
    "    src = src_batch.transpose(0, 1).to(DEVICE)\n",
    "    tgt_input = tgt_batch[:-1, :].transpose(1, 0).to(DEVICE)\n",
    "    tgt_output = tgt_batch[1:, :].transpose(1, 0).to(DEVICE)\n",
    "\n",
    "    input = [(src_item, tgt_item) for src_item, tgt_item in zip(src, tgt_input)]\n",
    "\n",
    "    return input, tgt_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_iter = Multi30k(\n",
    "    root=DATA_DIR,\n",
    "    split='train',\n",
    "    language_pair=(SRC_LANGUAGE, TGT_LANGUAGE), # type: ignore\n",
    ")\n",
    "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "val_iter = Multi30k(\n",
    "    root=DATA_DIR,\n",
    "    split='valid',\n",
    "    language_pair=(SRC_LANGUAGE, TGT_LANGUAGE), # type: ignore\n",
    ")\n",
    "val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_mind.supervised.handlers import Evaluator\n",
    "\n",
    "class MyEvaluator(Evaluator[str, str]):\n",
    "    def run(self, params) -> str:\n",
    "        model = typing.cast(Seq2SeqTransformer, params.model)\n",
    "        input = params.input\n",
    "        return self.translate(model=model, src_sentence=input)\n",
    "\n",
    "    # actual function to translate input sentence into target language\n",
    "    def translate(self, model: Seq2SeqTransformer, src_sentence: str):\n",
    "        model.eval()\n",
    "        src: torch.Tensor = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "        num_tokens = src.shape[0]\n",
    "        src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "\n",
    "        tgt_tokens = self.greedy_decode(\n",
    "            model=model,\n",
    "            src=src,\n",
    "            src_mask=src_mask,\n",
    "            max_len=num_tokens + 5,\n",
    "            start_symbol=BOS_IDX,\n",
    "        ).flatten()\n",
    "\n",
    "        return \" \".join(\n",
    "            vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))\n",
    "        ).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "\n",
    "    # function to generate output sequence using greedy algorithm\n",
    "    def greedy_decode(\n",
    "        self,\n",
    "        model: Seq2SeqTransformer,\n",
    "        src: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "        max_len: int,\n",
    "        start_symbol: int,\n",
    "    ):\n",
    "        src = src.to(DEVICE)\n",
    "        src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "        memory = model.encode(src, src_mask)\n",
    "        ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "        for i in range(max_len-1):\n",
    "            memory = memory.to(DEVICE)\n",
    "            tgt_mask = (\n",
    "                generate_square_subsequent_mask(ys.size(0)).type(torch.bool)\n",
    "            ).to(DEVICE)\n",
    "            out: torch.Tensor = model.decode(ys, memory, tgt_mask)\n",
    "            out = out.transpose(0, 1)\n",
    "            prob = model.generator(out[:, -1])\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "            next_word = next_word.item()\n",
    "\n",
    "            ys = torch.cat([\n",
    "                ys,\n",
    "                torch.ones(1, 1).type_as(src.data).fill_(next_word),\n",
    "            ], dim=0)\n",
    "            if next_word == EOS_IDX:\n",
    "                break\n",
    "        return ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_mind import supervised\n",
    "from auto_mind.supervised.handlers import BatchExecutor\n",
    "from src.lib.metrics import MainMetrics\n",
    "\n",
    "class MyBatchExecutor(BatchExecutor[list[tuple[torch.Tensor, torch.Tensor]], torch.Tensor]):\n",
    "    def run(self, params) -> torch.Tensor:\n",
    "        model = params.model\n",
    "        input = params.input\n",
    "\n",
    "        src = torch.stack([src for src, _ in input]).transpose(1, 0)\n",
    "        tgt_input = torch.stack([tgt_in for _, tgt_in in input]).transpose(1, 0)\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits: torch.Tensor = model(\n",
    "            src,\n",
    "            tgt_input,\n",
    "            src_mask,\n",
    "            tgt_mask,\n",
    "            src_padding_mask,\n",
    "            tgt_padding_mask,\n",
    "            src_padding_mask)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class MyLoss(torch.nn.Module):\n",
    "    def forward(self, output: torch.Tensor, target: torch.Tensor):\n",
    "        return loss_fn(output.reshape(-1, output.shape[-1]), target.transpose(1, 0).reshape(-1))\n",
    "\n",
    "manager = supervised.Manager(\n",
    "    data_params=supervised.ManagerDataParams(\n",
    "        train_dataloader=train_dataloader,\n",
    "        validation_dataloader=val_dataloader,\n",
    "        test_dataloader=None,\n",
    "    ),\n",
    "    model_params=supervised.ManagerModelParams(\n",
    "        model=transformer,\n",
    "        criterion=MyLoss(),\n",
    "        executor=MyBatchExecutor(),\n",
    "        use_best=False,\n",
    "    ),\n",
    "    optimizer_params=supervised.ManagerOptimizerParams(\n",
    "        optimizer=optimizer,\n",
    "        scheduler=None,\n",
    "        train_early_stopper=None,\n",
    "        test_early_stopper=None,\n",
    "    ),\n",
    "    metrics_params=supervised.ManagerMetricsParams(\n",
    "        evaluator=MyEvaluator(),\n",
    "        accuracy_calculator=None,\n",
    "        metrics_calculator=MainMetrics(name=NAME, no_accuracy=True, no_time=True).as_file_plotter(REPORT_PATH),\n",
    "        batch_interval=True,\n",
    "        default_interval=25,\n",
    "    ),\n",
    "    config=supervised.ManagerConfig(\n",
    "        save_path=SAVE_PATH,\n",
    "        random_seed=0,\n",
    "        train_hook=None,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.train(epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(manager.evaluate(\"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
