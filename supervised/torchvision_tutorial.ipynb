{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchVision Object Detection Finetuning Tutorial\n",
    "\n",
    "Based on the tutorial: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "\n",
    "This notebook provides a tutorial on finetuning object detection models using TorchVision in PyTorch. It demonstrates how to adapt a pre-trained Faster R-CNN model to detect objects in a custom dataset.\n",
    "\n",
    "The dataset is loaded using a custom `PennFudanDataset` class, which handles the loading and transformation of images and annotations. The data is then split into training and validation sets, which are loaded using `DataLoader` with a specified batch size.\n",
    "\n",
    "A pre-trained Faster R-CNN model is loaded using `torchvision.models.detection.fasterrcnn_resnet50_fpn`, and the final layers are modified to match the number of classes in the custom dataset. The model is trained using the SGD optimizer with momentum and a learning rate scheduler (`StepLR`) to adjust the learning rate at regular intervals.\n",
    "\n",
    "The training loop iterates over the dataset for a specified number of epochs, updating the model parameters to minimize the loss.\n",
    "\n",
    "Additionally, the notebook includes code to save the trained model's state dictionary to a file and load it back for inference. This allows for the model to be reused without retraining.\n",
    "\n",
    "The notebook concludes with a section on visualizing the model's predictions on a few validation images, providing a qualitative assessment of the model's performance.\n",
    "\n",
    "Most of the boilerplate for the training is handled by the manager class. This includes printing metrics during training and periodically saving the model, allowing training to be resumed from the last checkpoint.\n",
    "\n",
    "Normally, the loss is calculated using the model output and the target batch from the dataloader. In this tutorial, the target is not used directly to calculate the loss but is instead used as part of the arguments passed to the model during training. Therefore, the dataset was modified to return `((input, target), 0)` instead of just `(input, target)`, with `0` being an arbitrary value that won't be used. \n",
    "\n",
    "The collation function will return a tuple `(iterator[(input, target), ...], iterator[0, ...])`. The iterator is actually a tuple with the size `batch_size=2`, but it could also be a list with no difference in this case. The executor will receive the iterator of tuples `(input, target)`, create a list of inputs/images and another list of targets/dictionaries of tensors, pass them to the model, and return the output.\n",
    "\n",
    "A custom loss function was created to calculate the loss as the sum of the losses for each element in the output during training mode. For more details, see the section **Testing forward() method**, especially the output in training mode.\n",
    "\n",
    "The evaluator, `MyEvaluator`, receives an image as input and outputs an image (as a tensor). It is used at the end of this notebook to demonstrate what a prediction looks like with `output_image = manager.evaluate(image)`.\n",
    "\n",
    "The accuracy calculator class is currently set to always return 0.5 (50%). It can be modified to return an accuracy based on the input, target, and prediction/output. However, the focus of this example is to define a manager to train the model on the dataset and show what the predictions look like.\n",
    "\n",
    "A custom metrics class was used to calculate and display metrics related to losses, accuracy, and training times, which were then saved to a PDF file (`REPORT_PATH`). You can find implementation examples of various metrics classes in [lib/metrics.py](../lib/metrics.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = 'torchvision_tutorial'\n",
    "SAVE_PATH = f'data/test/train/{NAME}-checkpoint.pth'\n",
    "REPORT_PATH = f'data/test/train/{NAME}-report.pdf'\n",
    "DATA_PATH = 'data/penn_fudan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from src.datasets.penn_fudan_data import PennFudanData\n",
    "\n",
    "data = PennFudanData(root_path=DATA_PATH)\n",
    "datasets = data.datasets\n",
    "\n",
    "print(\n",
    "    \"Loading PennFudanData\",\n",
    "    len(datasets.train) if isinstance(datasets.train, typing.Sized) else None,\n",
    "    len(datasets.validation or []) if isinstance(datasets.validation, typing.Sized) else None,\n",
    "    len(datasets.test or []) if isinstance(datasets.test, typing.Sized) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.io import read_image\n",
    "\n",
    "image = read_image(f\"{data.main_data_dir}/PNGImages/FudanPed00046.png\")\n",
    "mask = read_image(f\"{data.main_data_dir}/PedMasks/FudanPed00046_mask.png\")\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Image\")\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.subplot(122)\n",
    "plt.title(\"Mask\")\n",
    "plt.imshow(mask.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 - Finetuning from a pretrained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - Modifying the model to add a different backbone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n",
    "# ``FasterRCNN`` needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280 # type: ignore\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32, 64, 128, 256, 512),),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),)\n",
    ")\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# ``OrderedDict[Tensor]``, and in ``featmap_names`` you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "    featmap_names=['0'],\n",
    "    output_size=7,\n",
    "    sampling_ratio=2\n",
    ")\n",
    "\n",
    "# put the pieces together inside a Faster-RCNN model\n",
    "model = FasterRCNN(\n",
    "    backbone,\n",
    "    num_classes=2,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Object detection and instance segmentation model for PennFudan Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "        in_features_mask,\n",
    "        hidden_layer,\n",
    "        num_classes\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Putting everything together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing forward() method (Optional)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision_tutorial import utils\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "dataset = datasets.train\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "# For Training\n",
    "images, targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "output = model(images, targets)  # Returns losses and detections\n",
    "print(output)\n",
    "\n",
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)  # Returns predictions\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataloaders, Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "datasets = data.datasets.transform(lambda data: (data, 0)).sized()\n",
    "\n",
    "def collate_fn(batch):\n",
    "    result = tuple(zip(*batch))\n",
    "    return result\n",
    "\n",
    "# define training and validation data loaders\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.train,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.validation,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.test,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "from auto_mind import supervised\n",
    "from auto_mind.supervised.handlers import Evaluator, BatchExecutor, BatchExecutorParams, BatchInOutParams\n",
    "from src.lib.metrics import MetricsListPlotter, MainMetrics, DatasetsAmountsMetrics, MetricsFileDirectPlotter\n",
    "\n",
    "Output = torch.Tensor | dict | list[dict]\n",
    "Input = list[tuple[torch.Tensor, Output]]\n",
    "\n",
    "class MyBatchExecutor(BatchExecutor[Input, Output]):\n",
    "    def run(self, params: BatchExecutorParams[Input]) -> Output:\n",
    "        images = list(image.to(device) for image, _ in params.input)\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for _, t in params.input]\n",
    "        loss_dict = model(images, targets)\n",
    "        return loss_dict\n",
    "\n",
    "def loss_fn(params: BatchInOutParams[Input, Output, torch.Tensor]) -> torch.Tensor:\n",
    "    output = params.output\n",
    "\n",
    "    if not torch.is_grad_enabled():\n",
    "        return torch.tensor(0.0)\n",
    "\n",
    "    if not isinstance(output, dict):\n",
    "        raise ValueError(\"output must be a dict\")\n",
    "\n",
    "    losses = sum(loss for loss in output.values())\n",
    "\n",
    "    # reduce losses over all GPUs for logging purposes\n",
    "    loss_dict_reduced = utils.reduce_dict(output)\n",
    "    losses_reduced: torch.Tensor = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "    loss_value = losses_reduced.item()\n",
    "\n",
    "    if not math.isfinite(loss_value):\n",
    "        print(f\"Loss is {loss_value}, stopping training\")\n",
    "        print(loss_dict_reduced)\n",
    "        sys.exit(1)\n",
    "\n",
    "    return losses\n",
    "\n",
    "class MyEvaluator(Evaluator[torch.Tensor, torch.Tensor]):\n",
    "    def run(self, params):\n",
    "        with torch.no_grad():\n",
    "            image = params.input\n",
    "            model = params.model\n",
    "\n",
    "            eval_transform = data.get_transform(train=False)\n",
    "            x = eval_transform(image)\n",
    "            # convert RGBA -> RGB and move to device\n",
    "            x = x[:3, ...].to(device)\n",
    "\n",
    "            model.eval()\n",
    "            predictions = model([x, ])\n",
    "            pred = predictions[0]\n",
    "\n",
    "            image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n",
    "            image = image[:3, ...]\n",
    "            pred_labels = [f\"pedestrian: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\n",
    "            pred_boxes = pred[\"boxes\"].long()\n",
    "            output_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\")\n",
    "\n",
    "            masks = (pred[\"masks\"] > 0.7).squeeze(1)\n",
    "            output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors=\"blue\")\n",
    "\n",
    "            return output_image\n",
    "\n",
    "manager = supervised.Manager(\n",
    "    data_params=supervised.ManagerDataParams(\n",
    "        train_dataloader=train_dataloader,\n",
    "        validation_dataloader=validation_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "    ),\n",
    "    model_params=supervised.ManagerModelParams(\n",
    "        model=model,\n",
    "        criterion=loss_fn,\n",
    "        executor=MyBatchExecutor(),\n",
    "        use_best=False,\n",
    "    ),\n",
    "    optimizer_params=supervised.ManagerOptimizerParams(\n",
    "        optimizer=optimizer,\n",
    "        scheduler=lr_scheduler,\n",
    "        train_early_stopper=None,\n",
    "        test_early_stopper=None,\n",
    "    ),\n",
    "    metrics_params=supervised.ManagerMetricsParams(\n",
    "        evaluator=MyEvaluator(),\n",
    "        accuracy_calculator=None,\n",
    "        metrics_calculator=MetricsFileDirectPlotter(\n",
    "            plotter=MetricsListPlotter(items=[\n",
    "                MainMetrics(name=NAME, no_accuracy=True),\n",
    "                DatasetsAmountsMetrics(name=f\"{NAME}_records\", datasets=datasets),\n",
    "            ]),\n",
    "            file_path=REPORT_PATH,\n",
    "            figsize=(8, 8),\n",
    "        ),\n",
    "        batch_interval=True,\n",
    "        default_interval=10,\n",
    "    ),\n",
    "    config=supervised.ManagerConfig(\n",
    "        save_path=SAVE_PATH,\n",
    "        random_seed=0,\n",
    "        train_hook=None,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.train(epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = read_image(f\"{data.main_data_dir}/PNGImages/FudanPed00046.png\")\n",
    "\n",
    "output_image = manager.evaluate(image)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(output_image.permute(1, 2, 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
